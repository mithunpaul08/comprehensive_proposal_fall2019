\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{fyodorov2000natural,condoravdi2003entailment,bos2005recognising,maccartney2009extended,dagan2013recognizing}
\citation{bowman2015large}
\citation{williams2017broad}
\citation{thorne2018fever}
\citation{pomerleau2017fake}
\citation{kim2018semantic}
\citation{gururangan2018annotation}
\citation{poliak2018hypothesis}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem Statement}{2}{section.1}}
\citation{gururangan2018annotation}
\citation{dagan2013recognizing}
\citation{parikh2016decomposable}
\citation{pomerleau2017fake}
\citation{parikh2016decomposable}
\citation{thorne2018fever}
\citation{pomerleau2017fake}
\citation{parikh2016decomposable}
\citation{chen2016enhanced}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary Work}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Experiment Set-Up}{5}{subsection.2.1}}
\citation{williams2017broad}
\citation{thorne2018fever}
\citation{pomerleau2017fake}
\citation{romanov2018lessons}
\citation{thorne2018fever}
\citation{thorne2018fever}
\citation{thorne2018fever}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Datasets}{6}{subsection.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Label distribution for the FEVER and FNC datasets. I consider the \textit  {agree} and \textit  {disagree} FNC labels as equivalent to the \textit  {support} and \textit  {refute} labels in FEVER. The FNC \textit  {not enough info (NEI)} label is listed below the more fine-grained \textit  {discuss} and \textit  {unrelated} FEVER labels. \relax }}{7}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:data}{{1}{7}{Label distribution for the FEVER and FNC datasets. I consider the \textit {agree} and \textit {disagree} FNC labels as equivalent to the \textit {support} and \textit {refute} labels in FEVER. The FNC \textit {not enough info (NEI)} label is listed below the more fine-grained \textit {discuss} and \textit {unrelated} FEVER labels. \relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Cross-domain Labels}{7}{subsubsection.2.2.1}}
\newlabel{sec:crossdomain}{{2.2.1}{7}{Cross-domain Labels}{subsubsection.2.2.1}{}}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Error Analysis of Cross-Domain Attention}{8}{subsection.2.3}}
\newlabel{attention_analysis}{{2.3}{8}{Error Analysis of Cross-Domain Attention}{subsection.2.3}{}}
\citation{zeman2008cross}
\citation{peyrard2019simple}
\citation{manning2014stanford}
\citation{ciaramita2003supersense}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Distribution of POS tags that were assigned the highest attention weights by decomposable attention for incorrectly classified cross-domain examples.\relax }}{9}{figure.caption.3}}
\newlabel{fig:attention}{{1}{9}{Distribution of POS tags that were assigned the highest attention weights by decomposable attention for incorrectly classified cross-domain examples.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Proposed Delexicalization Techniques}{9}{subsection.2.4}}
\citation{manning2014stanford}
\citation{pennington2014glove}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Masking Techniques}{10}{subsubsection.2.4.1}}
\newlabel{masking_techniques}{{2.4.1}{10}{Masking Techniques}{subsubsection.2.4.1}{}}
\citation{ciaramita2003supersense}
\citation{miller1990introduction}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Example illustratingmy various masking techniques, compared to the original fully lexicalized data. Note that the masking tags were generated with real-world (imperfect) tools. For example, "Airbus A380" in the claim was correctly classified as \texttt  {miscellaneous} by the NER tool, while ``A380" in the evidence was not, thus preventing us from taking advantage of the overlap. \relax }}{11}{table.caption.4}}
\newlabel{masking_examples}{{2}{11}{Example illustratingmy various masking techniques, compared to the original fully lexicalized data. Note that the masking tags were generated with real-world (imperfect) tools. For example, "Airbus A380" in the claim was correctly classified as \texttt {miscellaneous} by the NER tool, while ``A380" in the evidence was not, thus preventing us from taking advantage of the overlap. \relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Results}{12}{subsection.2.5}}
\newlabel{sec:results}{{2.5}{12}{Results}{subsection.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Phase 1}{12}{subsubsection.2.5.1}}
\citation{ling2012fine}
\citation{chen2016enhanced}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Various masking techniques and their performance accuracies, both in-domain and out-of-domain.\relax }}{13}{table.caption.5}}
\newlabel{crossdomain}{{3}{13}{Various masking techniques and their performance accuracies, both in-domain and out-of-domain.\relax }{table.caption.5}{}}
\newlabel{tab:results}{{3}{13}{Various masking techniques and their performance accuracies, both in-domain and out-of-domain.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Phase 2}{14}{subsubsection.2.5.2}}
\citation{emnlp2019sandeep}
\@writefile{toc}{\contentsline {section}{\numberline {3}Future Work/Plan for Dissertation}{15}{section.3}}
\citation{ba2014deep}
\citation{hinton2015distilling}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Performance of various high performing neural network methods over lexicalized and delexicalized versions of the same dataset. `Matched' is the in-domain partition of the MNLI validation dataset, and `mis-matched' is the out-of-domain partition. The performance of both the methods remain close to each other in delexicalized and lexicalized versions of the same dataset, which validates that my delexicalization techniques preserve the original information of the text. \relax }}{16}{table.caption.6}}
\newlabel{results}{{4}{16}{Performance of various high performing neural network methods over lexicalized and delexicalized versions of the same dataset. `Matched' is the in-domain partition of the MNLI validation dataset, and `mis-matched' is the out-of-domain partition. The performance of both the methods remain close to each other in delexicalized and lexicalized versions of the same dataset, which validates that my delexicalization techniques preserve the original information of the text. \relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Performance accuracies of the Decomposable Attention against various masking techniques when tested out-of-domain. The ``Train Domain'' row indicates the training datasets, while the ``Eval Domain'' indicates the domain of the corresponding evaluation partitions. For example, one experiment trained the decomposable attention method on FEVER and evaluated the resulting model on the testing partition of FNC (column 3). \relax }}{16}{table.caption.7}}
\newlabel{results_outofdomain}{{5}{16}{Performance accuracies of the Decomposable Attention against various masking techniques when tested out-of-domain. The ``Train Domain'' row indicates the training datasets, while the ``Eval Domain'' indicates the domain of the corresponding evaluation partitions. For example, one experiment trained the decomposable attention method on FEVER and evaluated the resulting model on the testing partition of FNC (column 3). \relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Knowledge Distillation of Data}{16}{subsection.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance accuracies of the Decomposable Attention against various masking techniques when tested in-domain for FNC and FEVER datasets. The ``Lexicalized" row shows the accuracies when decomposable attention was trained using the corresponding lexicalized data. This demonstrates that while delexicalization with OA-NER maintains the performance, the addition of Super Sense tags reduces the accuracy, emphasizing the fact that the amount of granularity to use is still an open problem.\relax }}{17}{table.caption.8}}
\newlabel{sstag}{{6}{17}{Performance accuracies of the Decomposable Attention against various masking techniques when tested in-domain for FNC and FEVER datasets. The ``Lexicalized" row shows the accuracies when decomposable attention was trained using the corresponding lexicalized data. This demonstrates that while delexicalization with OA-NER maintains the performance, the addition of Super Sense tags reduces the accuracy, emphasizing the fact that the amount of granularity to use is still an open problem.\relax }{table.caption.8}{}}
\citation{tzeng2015simultaneous}
\citation{papernot2016distillation}
\citation{sun2019patient}
\citation{jiao2019tinybert}
\citation{zhao2019extreme}
\citation{tang2019distilling}
\citation{liu2019attentive}
\citation{caruana1997multitask}
\citation{baxter2000model}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Related Work}{18}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Proposed Solution}{19}{subsection.3.3}}
\citation{jiao2019tinybert,tang2019distilling,zhao2019extreme}
\citation{parikh2016decomposable}
\citation{chen2016enhanced}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Proposed Experiment Set-Up}{20}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Experiment Variations}{20}{subsubsection.3.3.2}}
\bibstyle{unsrtnat}
\bibdata{compre_masking}
\bibcite{fyodorov2000natural}{{1}{2000}{{Fyodorov et~al.}}{{Fyodorov, Winter, and Francez}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{21}{section.4}}
\bibcite{condoravdi2003entailment}{{2}{2003}{{Condoravdi et~al.}}{{Condoravdi, Crouch, De~Paiva, Stolle, and Bobrow}}}
\bibcite{bos2005recognising}{{3}{2005}{{Bos and Markert}}{{}}}
\bibcite{maccartney2009extended}{{4}{2009}{{MacCartney and Manning}}{{}}}
\bibcite{dagan2013recognizing}{{5}{2013}{{Dagan et~al.}}{{Dagan, Roth, Sammons, and Zanzotto}}}
\bibcite{bowman2015large}{{6}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{williams2017broad}{{7}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{thorne2018fever}{{8}{2018}{{Thorne et~al.}}{{Thorne, Vlachos, Christodoulopoulos, and Mittal}}}
\bibcite{pomerleau2017fake}{{9}{2017}{{Pomerleau and Rao}}{{}}}
\bibcite{kim2018semantic}{{10}{2018}{{Kim et~al.}}{{Kim, Hong, Kang, and Kwak}}}
\bibcite{gururangan2018annotation}{{11}{2018}{{Gururangan et~al.}}{{Gururangan, Swayamdipta, Levy, Schwartz, Bowman, and Smith}}}
\bibcite{poliak2018hypothesis}{{12}{2018}{{Poliak et~al.}}{{Poliak, Naradowsky, Haldar, Rudinger, and Van~Durme}}}
\bibcite{parikh2016decomposable}{{13}{2016}{{Parikh et~al.}}{{Parikh, T{\"a}ckstr{\"o}m, Das, and Uszkoreit}}}
\bibcite{chen2016enhanced}{{14}{2016}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{romanov2018lessons}{{15}{2018}{{Romanov and Shivade}}{{}}}
\bibcite{bahdanau2014neural}{{16}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{zeman2008cross}{{17}{2008}{{Zeman and Resnik}}{{}}}
\bibcite{peyrard2019simple}{{18}{2019}{{Peyrard}}{{}}}
\bibcite{manning2014stanford}{{19}{2014}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{ciaramita2003supersense}{{20}{2003}{{Ciaramita and Johnson}}{{}}}
\bibcite{pennington2014glove}{{21}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{miller1990introduction}{{22}{1990}{{Miller et~al.}}{{Miller, Beckwith, Fellbaum, Gross, and Miller}}}
\bibcite{ling2012fine}{{23}{2012}{{Ling and Weld}}{{}}}
\bibcite{emnlp2019sandeep}{{24}{2019}{{Suntwal et~al.}}{{Suntwal, Paul, Sharp, and Surdeanu}}}
\bibcite{ba2014deep}{{25}{2014}{{Ba and Caruana}}{{}}}
\bibcite{hinton2015distilling}{{26}{2015}{{Hinton et~al.}}{{Hinton, Vinyals, and Dean}}}
\bibcite{tzeng2015simultaneous}{{27}{2015}{{Tzeng et~al.}}{{Tzeng, Hoffman, Darrell, and Saenko}}}
\bibcite{papernot2016distillation}{{28}{2016}{{Papernot et~al.}}{{Papernot, McDaniel, Wu, Jha, and Swami}}}
\bibcite{sun2019patient}{{29}{2019}{{Sun et~al.}}{{Sun, Cheng, Gan, and Liu}}}
\bibcite{jiao2019tinybert}{{30}{2019}{{Jiao et~al.}}{{Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and Liu}}}
\bibcite{zhao2019extreme}{{31}{2019}{{Zhao et~al.}}{{Zhao, Gupta, Song, and Zhou}}}
\bibcite{tang2019distilling}{{32}{2019}{{Tang et~al.}}{{Tang, Lu, Liu, Mou, Vechtomova, and Lin}}}
\bibcite{liu2019attentive}{{33}{2019}{{Liu et~al.}}{{Liu, Wang, Lin, Socher, and Xiong}}}
\bibcite{caruana1997multitask}{{34}{1997}{{Caruana}}{{}}}
\bibcite{baxter2000model}{{35}{2000}{{Baxter}}{{}}}
