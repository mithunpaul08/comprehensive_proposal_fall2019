\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{devlin2018bert,sun2018improving,bohnet2018morphosyntactic}
\citation{kim2018semantic}
\citation{baird2017talos}
\citation{nie2018combining}
\citation{fyodorov2000natural,condoravdi2003entailment,bos2005recognising,maccartney2009extended}
\citation{dagan2013recognizing}
\citation{thorne2018fever}
\citation{thorne2018fever}
\citation{gururangan2018annotation}
\citation{bowman2015large}
\citation{williams2017broad}
\citation{thorne2018fever}
\citation{pomerleau2017fake}
\citation{gururangan2018annotation}
\citation{poliak2018hypothesis}
\citation{schuster2019towards}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem Statement}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{1}{subsection.1.1}}
\citation{gururangan2018annotation}
\citation{emnlp2019sandeep}
\citation{dagan2013recognizing}
\citation{zeman2008cross}
\citation{peyrard2019simple}
\citation{zeman2008cross}
\citation{peyrard2019simple}
\citation{parikh2016decomposable}
\citation{thorne2018fever}
\citation{pomerleau2017fake}
\citation{parikh2016decomposable}
\citation{chen2016enhanced}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary Work}{2}{section.2}}
\citation{pomerleau2017fake}
\citation{thorne2018fever}
\citation{thorne2018fever}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Experiment Set-Up}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Datasets}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Cross-domain Labels}{3}{subsubsection.2.2.1}}
\newlabel{sec:crossdomain}{{2.2.1}{3}{Cross-domain Labels}{subsubsection.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Label distribution for the FEVER and FNC datasets. I consider the \textit  {agree} and \textit  {disagree} FNC labels as equivalent to the \textit  {support} and \textit  {refute} labels in FEVER. The FNC \textit  {not enough info (NEI)} label is listed below the more fine-grained \textit  {discuss} and \textit  {unrelated} FEVER labels. \relax }}{3}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:data}{{1}{3}{Label distribution for the FEVER and FNC datasets. I consider the \textit {agree} and \textit {disagree} FNC labels as equivalent to the \textit {support} and \textit {refute} labels in FEVER. The FNC \textit {not enough info (NEI)} label is listed below the more fine-grained \textit {discuss} and \textit {unrelated} FEVER labels. \relax }{table.caption.1}{}}
\citation{bahdanau2014neural}
\citation{zeman2008cross}
\citation{manning2014stanford}
\citation{ciaramita2003supersense}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Error Analysis of Cross-Domain Attention}{4}{subsection.2.3}}
\newlabel{attention_analysis}{{2.3}{4}{Error Analysis of Cross-Domain Attention}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Proposed Solutions}{4}{subsection.2.4}}
\citation{manning2014stanford}
\citation{pennington2014glove}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Distribution of POS tags that were assigned the highest attention weights by DA for incorrectly classified cross-domain examples.\relax }}{5}{figure.caption.2}}
\newlabel{fig:attention}{{1}{5}{Distribution of POS tags that were assigned the highest attention weights by DA for incorrectly classified cross-domain examples.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Masking Techniques}{5}{subsubsection.2.4.1}}
\newlabel{masking_techniques}{{2.4.1}{5}{Masking Techniques}{subsubsection.2.4.1}{}}
\citation{ciaramita2003supersense}
\citation{miller1990introduction}
\citation{thorne2018fever}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Example illustratingmy various masking techniques, compared to the original fully lexicalized data. Note that the masking tags were generated with real-world (imperfect) tools. For example, "Airbus A380" in the claim was correctly classified as \texttt  {miscellaneous} by the NER tool, while ``A380" in the evidence was not, thus preventing us from taking advantage of the overlap. \relax }}{6}{table.caption.3}}
\newlabel{masking_examples}{{2}{6}{Example illustratingmy various masking techniques, compared to the original fully lexicalized data. Note that the masking tags were generated with real-world (imperfect) tools. For example, "Airbus A380" in the claim was correctly classified as \texttt {miscellaneous} by the NER tool, while ``A380" in the evidence was not, thus preventing us from taking advantage of the overlap. \relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Various masking techniques and their performance accuracies, both in-domain and out-of-domain.\relax }}{6}{table.caption.4}}
\newlabel{crossdomain}{{3}{6}{Various masking techniques and their performance accuracies, both in-domain and out-of-domain.\relax }{table.caption.4}{}}
\newlabel{tab:results}{{3}{6}{Various masking techniques and their performance accuracies, both in-domain and out-of-domain.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Results}{6}{subsection.2.5}}
\newlabel{sec:results}{{2.5}{6}{Results}{subsection.2.5}{}}
\citation{emnlp2019sandeep}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Performance of various high performing NN methods over lexicalized and delexicalized versions of the same dataset. `Matched' is the in-domain partition of the MNLI validation dataset, and `mis-matched' is the out-of-domain partition. The performance of both the methods remain close to each other in delexicalized and lexicalized versions of the same dataset, which validates that my delexicalization techniques preserve the original information of the text. \relax }}{7}{table.caption.5}}
\newlabel{results}{{4}{7}{Performance of various high performing NN methods over lexicalized and delexicalized versions of the same dataset. `Matched' is the in-domain partition of the MNLI validation dataset, and `mis-matched' is the out-of-domain partition. The performance of both the methods remain close to each other in delexicalized and lexicalized versions of the same dataset, which validates that my delexicalization techniques preserve the original information of the text. \relax }{table.caption.5}{}}
\citation{ba2014deep,hinton2015distilling}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Performance accuracies of the Decomposable Attention against various masking techniques when tested out-of-domain. The ``Train Domain'' row indicates the training datasets, while the ``Eval Domain'' indicates the domain of the corresponding evaluation partitions. For example, one experiment trained the DA method on FEVER and evaluated the resulting model on the testing partition of FNC (column 3). \relax }}{8}{table.caption.6}}
\newlabel{results_outofdomain}{{5}{8}{Performance accuracies of the Decomposable Attention against various masking techniques when tested out-of-domain. The ``Train Domain'' row indicates the training datasets, while the ``Eval Domain'' indicates the domain of the corresponding evaluation partitions. For example, one experiment trained the DA method on FEVER and evaluated the resulting model on the testing partition of FNC (column 3). \relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance accuracies of the Decomposable Attention against various masking techniques when tested in-domain for FNC and FEVER datasets. The ``Lexicalized" row shows the accuracies when DA was trained using the corresponding lexicalized data. This demonstrates that while delexicalization with OA-NER maintains the performance, the addition of Super Sense tags reduces the accuracy, emphasizing the fact that the amount of granularity to use is still an open problem.\relax }}{8}{table.caption.7}}
\newlabel{sstag}{{6}{8}{Performance accuracies of the Decomposable Attention against various masking techniques when tested in-domain for FNC and FEVER datasets. The ``Lexicalized" row shows the accuracies when DA was trained using the corresponding lexicalized data. This demonstrates that while delexicalization with OA-NER maintains the performance, the addition of Super Sense tags reduces the accuracy, emphasizing the fact that the amount of granularity to use is still an open problem.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Future Work/Plan for Dissertation}{8}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Knowledge Distillation of Data}{8}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Discussion of learning temperatures}{8}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Proposed Solution}{8}{subsection.3.3}}
\citation{jiao2019tinybert,tang2019distilling,zhao2019extreme}
\citation{parikh2016decomposable}
\citation{chen2016enhanced}
\citation{seo2016bidirectional}
\citation{jiao2019tinybert}
\citation{zhao2019extreme}
\citation{tang2019distilling}
\citation{liu2019attentive}
\citation{caruana1997multitask}
\citation{baxter2000model}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Related Work}{9}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Proposed Experiment Set-Up}{9}{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Experiment Variations}{9}{subsubsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}EMA}{9}{subsubsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Semi Supervised}{9}{subsubsection.3.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.5}Weight Tuning}{9}{subsubsection.3.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Mean teacher}{9}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}domain adaptation }{9}{subsection.3.6}}
\bibstyle{unsrtnat}
\bibdata{compre_masking}
\bibcite{devlin2018bert}{{1}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{sun2018improving}{{2}{2018}{{Sun et~al.}}{{Sun, Yu, Yu, and Cardie}}}
\bibcite{bohnet2018morphosyntactic}{{3}{2018}{{Bohnet et~al.}}{{Bohnet, McDonald, Simoes, Andor, Pitler, and Maynez}}}
\bibcite{kim2018semantic}{{4}{2018}{{Kim et~al.}}{{Kim, Hong, Kang, and Kwak}}}
\bibcite{baird2017talos}{{5}{2017}{{Baird et~al.}}{{Baird, Sibley, and Pan}}}
\bibcite{nie2018combining}{{6}{2018}{{Nie et~al.}}{{Nie, Chen, and Bansal}}}
\bibcite{fyodorov2000natural}{{7}{2000}{{Fyodorov et~al.}}{{Fyodorov, Winter, and Francez}}}
\bibcite{condoravdi2003entailment}{{8}{2003}{{Condoravdi et~al.}}{{Condoravdi, Crouch, De~Paiva, Stolle, and Bobrow}}}
\bibcite{bos2005recognising}{{9}{2005}{{Bos and Markert}}{{}}}
\bibcite{maccartney2009extended}{{10}{2009}{{MacCartney and Manning}}{{}}}
\bibcite{dagan2013recognizing}{{11}{2013}{{Dagan et~al.}}{{Dagan, Roth, Sammons, and Zanzotto}}}
\bibcite{thorne2018fever}{{12}{2018}{{Thorne et~al.}}{{Thorne, Vlachos, Christodoulopoulos, and Mittal}}}
\bibcite{gururangan2018annotation}{{13}{2018}{{Gururangan et~al.}}{{Gururangan, Swayamdipta, Levy, Schwartz, Bowman, and Smith}}}
\bibcite{bowman2015large}{{14}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{williams2017broad}{{15}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Amount of generalization}{10}{subsection.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}{section.4}}
\bibcite{pomerleau2017fake}{{16}{2017}{{Pomerleau and Rao}}{{}}}
\bibcite{poliak2018hypothesis}{{17}{2018}{{Poliak et~al.}}{{Poliak, Naradowsky, Haldar, Rudinger, and Van~Durme}}}
\bibcite{schuster2019towards}{{18}{2019}{{Schuster et~al.}}{{Schuster, Shah, Yeo, Filizzola, Santus, and Barzilay}}}
\bibcite{emnlp2019sandeep}{{19}{2019}{{Suntwal et~al.}}{{Suntwal, Paul, Sharp, and Surdeanu}}}
\bibcite{zeman2008cross}{{20}{2008}{{Zeman and Resnik}}{{}}}
\bibcite{peyrard2019simple}{{21}{2019}{{Peyrard}}{{}}}
\bibcite{parikh2016decomposable}{{22}{2016}{{Parikh et~al.}}{{Parikh, T{\"a}ckstr{\"o}m, Das, and Uszkoreit}}}
\bibcite{chen2016enhanced}{{23}{2016}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{bahdanau2014neural}{{24}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{manning2014stanford}{{25}{2014}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{ciaramita2003supersense}{{26}{2003}{{Ciaramita and Johnson}}{{}}}
\bibcite{pennington2014glove}{{27}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{miller1990introduction}{{28}{1990}{{Miller et~al.}}{{Miller, Beckwith, Fellbaum, Gross, and Miller}}}
\bibcite{ba2014deep}{{29}{2014}{{Ba and Caruana}}{{}}}
\bibcite{hinton2015distilling}{{30}{2015}{{Hinton et~al.}}{{Hinton, Vinyals, and Dean}}}
\bibcite{seo2016bidirectional}{{31}{2016}{{Seo et~al.}}{{Seo, Kembhavi, Farhadi, and Hajishirzi}}}
\bibcite{jiao2019tinybert}{{32}{2019}{{Jiao et~al.}}{{Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and Liu}}}
\bibcite{zhao2019extreme}{{33}{2019}{{Zhao et~al.}}{{Zhao, Gupta, Song, and Zhou}}}
\bibcite{tang2019distilling}{{34}{2019}{{Tang et~al.}}{{Tang, Lu, Liu, Mou, Vechtomova, and Lin}}}
\bibcite{liu2019attentive}{{35}{2019}{{Liu et~al.}}{{Liu, Wang, Lin, Socher, and Xiong}}}
\bibcite{caruana1997multitask}{{36}{1997}{{Caruana}}{{}}}
\bibcite{baxter2000model}{{37}{2000}{{Baxter}}{{}}}
