\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Fyodorov et~al.(2000)Fyodorov, Winter, and
  Francez]{fyodorov2000natural}
Yaroslav Fyodorov, Yoad Winter, and Nissim Francez.
\newblock A natural logic inference system.
\newblock In \emph{Proceedings of the 2nd Workshop on Inference in
  Computational Semantics (ICoS-2)}. Citeseer, 2000.

\bibitem[Condoravdi et~al.(2003)Condoravdi, Crouch, De~Paiva, Stolle, and
  Bobrow]{condoravdi2003entailment}
Cleo Condoravdi, Dick Crouch, Valeria De~Paiva, Reinhard Stolle, and Daniel~G
  Bobrow.
\newblock Entailment, intensionality and text understanding.
\newblock In \emph{Proceedings of the HLT-NAACL 2003 workshop on Text meaning},
  pages 38--45, 2003.

\bibitem[Bos and Markert(2005)]{bos2005recognising}
Johan Bos and Katja Markert.
\newblock Recognising textual entailment with logical inference.
\newblock In \emph{Proceedings of the conference on Human Language Technology
  and Empirical Methods in Natural Language Processing}, pages 628--635.
  Association for Computational Linguistics, 2005.

\bibitem[MacCartney and Manning(2009)]{maccartney2009extended}
Bill MacCartney and Christopher~D Manning.
\newblock An extended model of natural logic.
\newblock In \emph{Proceedings of the eighth international conference on
  computational semantics}, pages 140--156. Association for Computational
  Linguistics, 2009.

\bibitem[Dagan et~al.(2013)Dagan, Roth, Sammons, and
  Zanzotto]{dagan2013recognizing}
Ido Dagan, Dan Roth, Mark Sammons, and Fabio~Massimo Zanzotto.
\newblock Recognizing textual entailment: Models and applications.
\newblock \emph{Synthesis Lectures on Human Language Technologies}, 6\penalty0
  (4):\penalty0 1--220, 2013.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock \emph{arXiv preprint arXiv:1508.05326}, 2015.

\bibitem[Williams et~al.(2017)Williams, Nangia, and Bowman]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}, 2017.

\bibitem[Thorne et~al.(2018)Thorne, Vlachos, Christodoulopoulos, and
  Mittal]{thorne2018fever}
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
\newblock Fever: a large-scale dataset for fact extraction and verification.
\newblock \emph{arXiv preprint arXiv:1803.05355}, 2018.

\bibitem[Khot et~al.(2018)Khot, Sabharwal, and Clark]{khot2018scitail}
Tushar Khot, Ashish Sabharwal, and Peter Clark.
\newblock Scitail: A textual entailment dataset from science question
  answering.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Pomerleau and Rao(2017)]{pomerleau2017fake}
Dean Pomerleau and Delip Rao.
\newblock Fake news challenge, 2017.

\bibitem[Kim et~al.(2018)Kim, Hong, Kang, and Kwak]{kim2018semantic}
Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and Nojun Kwak.
\newblock Semantic sentence matching with densely-connected recurrent and
  co-attentive information.
\newblock \emph{arXiv preprint arXiv:1805.11360}, 2018.

\bibitem[Chen et~al.(2016)Chen, Zhu, Ling, Wei, Jiang, and
  Inkpen]{chen2016enhanced}
Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si~Wei, Hui Jiang, and Diana Inkpen.
\newblock Enhanced lstm for natural language inference.
\newblock \emph{arXiv preprint arXiv:1609.06038}, 2016.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, He, Chen, and Gao]{liu2019multi}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1901.11504}, 2019{\natexlab{a}}.

\bibitem[Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz,
  Bowman, and Smith]{gururangan2018annotation}
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel~R
  Bowman, and Noah~A Smith.
\newblock Annotation artifacts in natural language inference data.
\newblock \emph{arXiv preprint arXiv:1803.02324}, 2018.

\bibitem[Poliak et~al.(2018)Poliak, Naradowsky, Haldar, Rudinger, and
  Van~Durme]{poliak2018hypothesis}
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin
  Van~Durme.
\newblock Hypothesis only baselines in natural language inference.
\newblock \emph{arXiv preprint arXiv:1805.01042}, 2018.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh2016decomposable}
Ankur~P Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock \emph{arXiv preprint arXiv:1606.01933}, 2016.

\bibitem[Baird et~al.(2017)Baird, Sibley, and Pan]{baird2017talos}
Sean Baird, Doug Sibley, and Yuxi Pan.
\newblock Talos targets disinformation with fake news challenge victory.
\newblock \emph{Fake News Challenge}, 2017.

\bibitem[Nie et~al.(2019)Nie, Chen, and Bansal]{nie2019combining}
Yixin Nie, Haonan Chen, and Mohit Bansal.
\newblock Combining fact extraction and verification with neural semantic
  matching networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 6859--6866, 2019.

\bibitem[Lorenz(2002)]{lorenz2002really}
Gunter Lorenz.
\newblock Really worthwhile or not really significant? a corpus-based approach
  to the delexicalization.
\newblock \emph{New reflections on grammaticalization}, 49:\penalty0 143, 2002.

\bibitem[Zeman and Resnik(2008)]{zeman2008cross}
Daniel Zeman and Philip Resnik.
\newblock Cross-language parser adaptation between related languages.
\newblock In \emph{Proceedings of the IJCNLP-08 Workshop on NLP for Less
  Privileged Languages}, 2008.

\bibitem[Szarvas et~al.(2013)Szarvas, Biemann, and
  Gurevych]{szarvas2013supervised}
Gy{\"o}rgy Szarvas, Chris Biemann, and Iryna Gurevych.
\newblock Supervised all-words lexical substitution using delexicalized
  features.
\newblock In \emph{Proceedings of the 2013 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1131--1141, 2013.

\bibitem[Shin et~al.(2018)Shin, Yoo, and Lee]{shin2018slot}
Youhyun Shin, Kang~Min Yoo, and Sang-goo Lee.
\newblock Slot filling with delexicalized sentence generation.
\newblock In \emph{Interspeech}, pages 2082--2086, 2018.

\bibitem[Herzig and Berant(2018)]{herzig2018decoupling}
Jonathan Herzig and Jonathan Berant.
\newblock Decoupling structure and lexicon for zero-shot semantic parsing.
\newblock \emph{arXiv preprint arXiv:1804.07918}, 2018.

\bibitem[Peyrard(2019)]{peyrard2019simple}
Maxime Peyrard.
\newblock A simple theoretical model of importance for summarization.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 1059--1073, 2019.

\bibitem[Ba and Caruana(2014)]{ba2014deep}
Jimmy Ba and Rich Caruana.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in neural information processing systems}, pages
  2654--2662, 2014.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Tzeng et~al.(2015)Tzeng, Hoffman, Darrell, and
  Saenko]{tzeng2015simultaneous}
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko.
\newblock Simultaneous deep transfer across domains and tasks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 4068--4076, 2015.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pages
  582--597. IEEE, 2016.

\bibitem[Sun et~al.(2019)Sun, Cheng, Gan, and Liu]{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for bert model compression.
\newblock \emph{arXiv preprint arXiv:1908.09355}, 2019.

\bibitem[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1909.10351}, 2019.

\bibitem[Zhao et~al.(2019)Zhao, Gupta, Song, and Zhou]{zhao2019extreme}
Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou.
\newblock Extreme language model compression with optimal subwords and shared
  projections.
\newblock \emph{arXiv preprint arXiv:1909.11687}, 2019.

\bibitem[Tang et~al.(2019)Tang, Lu, Liu, Mou, Vechtomova, and
  Lin]{tang2019distilling}
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock \emph{arXiv preprint arXiv:1903.12136}, 2019.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Wang, Lin, Socher, and
  Xiong]{liu2019attentive}
Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, and Caiming Xiong.
\newblock Attentive student meets multi-task teacher: Improved knowledge
  distillation for pretrained models.
\newblock \emph{arXiv preprint arXiv:1911.03588}, 2019{\natexlab{b}}.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Romanov and Shivade(2018)]{romanov2018lessons}
Alexey Romanov and Chaitanya Shivade.
\newblock Lessons from natural language inference in the clinical domain.
\newblock \emph{arXiv preprint arXiv:1808.06752}, 2018.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Manning et~al.(2014)Manning, Surdeanu, Bauer, Finkel, Bethard, and
  McClosky]{manning2014stanford}
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard,
  and David McClosky.
\newblock The stanford corenlp natural language processing toolkit.
\newblock In \emph{Proceedings of 52nd annual meeting of the association for
  computational linguistics: system demonstrations}, pages 55--60, 2014.

\bibitem[Ciaramita and Johnson(2003)]{ciaramita2003supersense}
Massimiliano Ciaramita and Mark Johnson.
\newblock Supersense tagging of unknown nouns in wordnet.
\newblock In \emph{Proceedings of the 2003 conference on Empirical methods in
  natural language processing}, pages 168--175. Association for Computational
  Linguistics, 2003.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 1532--1543, 2014.

\bibitem[Miller et~al.(1990)Miller, Beckwith, Fellbaum, Gross, and
  Miller]{miller1990introduction}
George~A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and
  Katherine~J Miller.
\newblock Introduction to wordnet: An on-line lexical database.
\newblock \emph{International journal of lexicography}, 3\penalty0
  (4):\penalty0 235--244, 1990.

\bibitem[Ling and Weld(2012)]{ling2012fine}
Xiao Ling and Daniel~S Weld.
\newblock Fine-grained entity recognition.
\newblock In \emph{Twenty-Sixth AAAI Conference on Artificial Intelligence},
  2012.

\bibitem[Suntwal et~al.(2019{\natexlab{a}})Suntwal, Paul, Sharp, and
  Surdeanu]{suntwal2019importance}
Sandeep Suntwal, Mithun Paul, Rebecca Sharp, and Mihai Surdeanu.
\newblock On the importance of delexicalization for fact verification.
\newblock \emph{arXiv preprint arXiv:1909.09868}, 2019{\natexlab{a}}.

\bibitem[Suntwal et~al.(2019{\natexlab{b}})Suntwal, Paul, Sharp, and
  Surdeanu]{emnlp2019sandeep}
Sandeep Suntwal, Mithun Paul, Rebecca Sharp, and Mihai Surdeanu.
\newblock On the importance of delexicalization for fact verification.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing, (Short Papers)}, Hong Kong, November
  2019{\natexlab{b}}. Association for Computational Linguistics.
\newblock URL
  \url{http://clulab.cs.arizona.edu/papers/emnlp_2019_masking_suntwal_etal.pdf}.

\bibitem[Sharp et~al.(2019)Sharp, Pyarelal, Gyori, Alcock, Laparra,
  Valenzuela-Escarcega, Nagesh, Yadav, Bachman, Tang, Lent, Luo, Paul, Bethard,
  Barnard, Morrison, and Surdeanu]{sharp}
Rebecca Sharp, Adarsh Pyarelal, Benjamin Gyori, Keith Alcock, Egoitz Laparra,
  Marco~A. Valenzuela-Escarcega, Ajay Nagesh, Vikas Yadav, John Bachman, Zheng
  Tang, Heather Lent, Fan Luo, Mithun Paul, Steven Bethard, Kobus Barnard,
  Clayton Morrison, and Mihai Surdeanu.
\newblock Eidos, indra, \& delphi: From free text to executable causal models.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages 42--47, Minneapolis, Minnesota, 6 2019. Association for Computational
  Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/N19-4008}.

\bibitem[Sharp et~al.(2018)Sharp, Paul, Nagesh, Bell, and
  Surdeanu]{sharpGrounding}
Rebecca Sharp, Mithun Paul, Ajay Nagesh, Dane Bell, and Mihai Surdeanu.
\newblock Grounding gradable adjectives through crowdsourcing.
\newblock In Nicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry
  Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph
  Mariani, H\'{e}l\`{e}ne Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis,
  and Takenobu Tokunaga, editors, \emph{Proceedings of the Eleventh
  International Conference on Language Resources and Evaluation (LREC 2018)},
  Paris, France, May 2018. European Language Resources Association (ELRA).
\newblock ISBN 979-10-95546-00-9.
\newblock URL \url{http://www.lrec-conf.org/proceedings/lrec2018/pdf/977.pdf}.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In \emph{Advances in neural information processing systems}, pages
  1195--1204, 2017.

\bibitem[Valpola(2015)]{valpola2015neural}
Harri Valpola.
\newblock From neural pca to deep unsupervised learning.
\newblock In \emph{Advances in Independent Component Analysis and Learning
  Machines}, pages 143--171. Elsevier, 2015.

\bibitem[Rasmus et~al.(2015)Rasmus, Berglund, Honkala, Valpola, and
  Raiko]{rasmus2015semi}
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko.
\newblock Semi-supervised learning with ladder networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3546--3554, 2015.

\end{thebibliography}
