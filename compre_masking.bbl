\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Sun et~al.(2018)Sun, Yu, Yu, and Cardie]{sun2018improving}
Kai Sun, Dian Yu, Dong Yu, and Claire Cardie.
\newblock Improving machine reading comprehension with general reading
  strategies.
\newblock \emph{arXiv preprint arXiv:1810.13441}, 2018.

\bibitem[Bohnet et~al.(2018)Bohnet, McDonald, Simoes, Andor, Pitler, and
  Maynez]{bohnet2018morphosyntactic}
Bernd Bohnet, Ryan McDonald, Goncalo Simoes, Daniel Andor, Emily Pitler, and
  Joshua Maynez.
\newblock Morphosyntactic tagging with a meta-bilstm model over context
  sensitive token encodings.
\newblock \emph{arXiv preprint arXiv:1805.08237}, 2018.

\bibitem[Kim et~al.(2018)Kim, Hong, Kang, and Kwak]{kim2018semantic}
Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and Nojun Kwak.
\newblock Semantic sentence matching with densely-connected recurrent and
  co-attentive information.
\newblock \emph{arXiv preprint arXiv:1805.11360}, 2018.

\bibitem[Baird et~al.(2017)Baird, Sibley, and Pan]{baird2017talos}
Sean Baird, Doug Sibley, and Yuxi Pan.
\newblock Talos targets disinformation with fake news challenge victory.
\newblock \emph{Fake News Challenge}, 2017.

\bibitem[Nie et~al.(2018)Nie, Chen, and Bansal]{nie2018combining}
Yixin Nie, Haonan Chen, and Mohit Bansal.
\newblock Combining fact extraction and verification with neural semantic
  matching networks.
\newblock \emph{arXiv preprint arXiv:1811.07039}, 2018.

\bibitem[Fyodorov et~al.(2000)Fyodorov, Winter, and
  Francez]{fyodorov2000natural}
Yaroslav Fyodorov, Yoad Winter, and Nissim Francez.
\newblock A natural logic inference system.
\newblock In \emph{Proceedings of the 2nd Workshop on Inference in
  Computational Semantics (ICoS-2)}. Citeseer, 2000.

\bibitem[Condoravdi et~al.(2003)Condoravdi, Crouch, De~Paiva, Stolle, and
  Bobrow]{condoravdi2003entailment}
Cleo Condoravdi, Dick Crouch, Valeria De~Paiva, Reinhard Stolle, and Daniel~G
  Bobrow.
\newblock Entailment, intensionality and text understanding.
\newblock In \emph{Proceedings of the HLT-NAACL 2003 workshop on Text meaning},
  pages 38--45, 2003.

\bibitem[Bos and Markert(2005)]{bos2005recognising}
Johan Bos and Katja Markert.
\newblock Recognising textual entailment with logical inference.
\newblock In \emph{Proceedings of the conference on Human Language Technology
  and Empirical Methods in Natural Language Processing}, pages 628--635.
  Association for Computational Linguistics, 2005.

\bibitem[MacCartney and Manning(2009)]{maccartney2009extended}
Bill MacCartney and Christopher~D Manning.
\newblock An extended model of natural logic.
\newblock In \emph{Proceedings of the eighth international conference on
  computational semantics}, pages 140--156. Association for Computational
  Linguistics, 2009.

\bibitem[Dagan et~al.(2013)Dagan, Roth, Sammons, and
  Zanzotto]{dagan2013recognizing}
Ido Dagan, Dan Roth, Mark Sammons, and Fabio~Massimo Zanzotto.
\newblock Recognizing textual entailment: Models and applications.
\newblock \emph{Synthesis Lectures on Human Language Technologies}, 6\penalty0
  (4):\penalty0 1--220, 2013.

\bibitem[Thorne et~al.(2018)Thorne, Vlachos, Christodoulopoulos, and
  Mittal]{thorne2018fever}
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
\newblock Fever: a large-scale dataset for fact extraction and verification.
\newblock \emph{arXiv preprint arXiv:1803.05355}, 2018.

\bibitem[Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz,
  Bowman, and Smith]{gururangan2018annotation}
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel~R
  Bowman, and Noah~A Smith.
\newblock Annotation artifacts in natural language inference data.
\newblock \emph{arXiv preprint arXiv:1803.02324}, 2018.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock \emph{arXiv preprint arXiv:1508.05326}, 2015.

\bibitem[Williams et~al.(2017)Williams, Nangia, and Bowman]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}, 2017.

\bibitem[Pomerleau and Rao(2017)]{pomerleau2017fake}
Dean Pomerleau and Delip Rao.
\newblock Fake news challenge, 2017.

\bibitem[Poliak et~al.(2018)Poliak, Naradowsky, Haldar, Rudinger, and
  Van~Durme]{poliak2018hypothesis}
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin
  Van~Durme.
\newblock Hypothesis only baselines in natural language inference.
\newblock \emph{arXiv preprint arXiv:1805.01042}, 2018.

\bibitem[Schuster et~al.(2019)Schuster, Shah, Yeo, Filizzola, Santus, and
  Barzilay]{schuster2019towards}
Tal Schuster, Darsh~J Shah, Yun Jie~Serene Yeo, Daniel Filizzola, Enrico
  Santus, and Regina Barzilay.
\newblock Towards debiasing fact verification models.
\newblock \emph{arXiv preprint arXiv:1908.05267}, 2019.

\bibitem[Suntwal et~al.(2019)Suntwal, Paul, Sharp, and
  Surdeanu]{emnlp2019sandeep}
Sandeep Suntwal, Mithun Paul, Rebecca Sharp, and Mihai Surdeanu.
\newblock On the importance of delexicalization for fact verification.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing, (Short Papers)}, Hong Kong, November 2019.
  Association for Computational Linguistics.
\newblock URL
  \url{http://clulab.cs.arizona.edu/papers/emnlp_2019_masking_suntwal_etal.pdf}.

\bibitem[Zeman and Resnik(2008)]{zeman2008cross}
Daniel Zeman and Philip Resnik.
\newblock Cross-language parser adaptation between related languages.
\newblock In \emph{Proceedings of the IJCNLP-08 Workshop on NLP for Less
  Privileged Languages}, 2008.

\bibitem[Peyrard(2019)]{peyrard2019simple}
Maxime Peyrard.
\newblock A simple theoretical model of importance for summarization.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 1059--1073, 2019.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh2016decomposable}
Ankur~P Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock \emph{arXiv preprint arXiv:1606.01933}, 2016.

\bibitem[Chen et~al.(2016)Chen, Zhu, Ling, Wei, Jiang, and
  Inkpen]{chen2016enhanced}
Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si~Wei, Hui Jiang, and Diana Inkpen.
\newblock Enhanced lstm for natural language inference.
\newblock \emph{arXiv preprint arXiv:1609.06038}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Manning et~al.(2014)Manning, Surdeanu, Bauer, Finkel, Bethard, and
  McClosky]{manning2014stanford}
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard,
  and David McClosky.
\newblock The stanford corenlp natural language processing toolkit.
\newblock In \emph{Proceedings of 52nd annual meeting of the association for
  computational linguistics: system demonstrations}, pages 55--60, 2014.

\bibitem[Ciaramita and Johnson(2003)]{ciaramita2003supersense}
Massimiliano Ciaramita and Mark Johnson.
\newblock Supersense tagging of unknown nouns in wordnet.
\newblock In \emph{Proceedings of the 2003 conference on Empirical methods in
  natural language processing}, pages 168--175. Association for Computational
  Linguistics, 2003.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 1532--1543, 2014.

\bibitem[Miller et~al.(1990)Miller, Beckwith, Fellbaum, Gross, and
  Miller]{miller1990introduction}
George~A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and
  Katherine~J Miller.
\newblock Introduction to wordnet: An on-line lexical database.
\newblock \emph{International journal of lexicography}, 3\penalty0
  (4):\penalty0 235--244, 1990.

\bibitem[Ba and Caruana(2014)]{ba2014deep}
Jimmy Ba and Rich Caruana.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in neural information processing systems}, pages
  2654--2662, 2014.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Seo et~al.(2016)Seo, Kembhavi, Farhadi, and
  Hajishirzi]{seo2016bidirectional}
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi.
\newblock Bidirectional attention flow for machine comprehension.
\newblock \emph{arXiv preprint arXiv:1611.01603}, 2016.

\bibitem[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1909.10351}, 2019.

\bibitem[Zhao et~al.(2019)Zhao, Gupta, Song, and Zhou]{zhao2019extreme}
Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou.
\newblock Extreme language model compression with optimal subwords and shared
  projections.
\newblock \emph{arXiv preprint arXiv:1909.11687}, 2019.

\bibitem[Tang et~al.(2019)Tang, Lu, Liu, Mou, Vechtomova, and
  Lin]{tang2019distilling}
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock \emph{arXiv preprint arXiv:1903.12136}, 2019.

\bibitem[Liu et~al.(2019)Liu, Wang, Lin, Socher, and Xiong]{liu2019attentive}
Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, and Caiming Xiong.
\newblock Attentive student meets multi-task teacher: Improved knowledge
  distillation for pretrained models.
\newblock \emph{arXiv preprint arXiv:1911.03588}, 2019.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\end{thebibliography}
